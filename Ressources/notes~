http://docs.opencv.org/modules/contrib/doc/facerec/facerec_api.html
SE RENSEIGNER SUR LES CLASSIFIERS

All face recognition models in OpenCV are derived from the abstract base class FaceRecognizer, which provides a unified access to all face recongition algorithms in OpenCV.


    Training of a FaceRecognizer with FaceRecognizer::train() on a given set of images (your face database!).
    Prediction of a given sample image, that means a face. The image is given as a Mat.
    Loading/Saving the model state from/to a given XML or YAML.


Python3 mal supporté

FaceRecognizer::train

Trains a FaceRecognizer with given data and associated labels.
Il faut le faire sur des images en n&b & sur des images de même taille

C++: void FaceRecognizer::train(InputArrayOfArrays src, InputArray labels) = 0
    Parameters:	

        src – The training images, that means the faces you want to learn. The data has to be given as a vector<Mat>.
        labels – The labels corresponding to the images have to be given either as a vector<int> or a
update seulement pour LBPH (pas eigen ni fisher)

FaceRecognizer::save

Saves a FaceRecognizer and its model state.

createEigenFaceRecognizer

C++: Ptr<FaceRecognizer> createEigenFaceRecognizer(int num_components=0, double threshold=DBL_MAX)
    Parameters:	

        num_components – The number of components (read: Eigenfaces) kept for this Prinicpal Component Analysis. As a hint: There’s no rule how many components (read: Eigenfaces) should be kept for good reconstruction capabilities. It is based on your input data, so experiment with the number. Keeping 80 components should almost always be sufficient.
        threshold – The threshold applied in the prediciton.


http://docs.opencv.org/modules/contrib/doc/facerec/facerec_tutorial.html
History : 
OpenCV (Open Source Computer Vision) is a popular computer vision library started by Intel in 1999. The cross-platform library sets its focus on real-time image processing and includes patent-free implementations of the latest computer vision algorithms. In 2008 Willow Garage took over support and OpenCV 2.3.1 now comes with a programming interface to C, C++, Python and Android. OpenCV is released under a BSD license so it is used in academic projects and commercial products alike.

It was shown by David Hubel and Torsten Wiesel, that our brain has specialized nerve cells responding to specific local features of a scene, such as lines, edges, angles or movement

Face recognition based on the geometric features of a face is probably the most intuitive approach to face recognition. One of the first automated face recognition systems was described in [Kanade73]: marker points (position of eyes, ears, nose, ...) were used to build a feature vector (distance between the points, angle between them, ...). The recognition was performed by calculating the euclidean distance between feature vectors of a probe and reference image. Such a method is robust against changes in illumination by its nature, but has a huge drawback: the accurate registration of the marker points is complicated, even with state of the art algorithms. 

The Eigenfaces method described in [TP91] took a holistic approach to face recognition: A facial image is a point from a high-dimensional image space and a lower-dimensional representation is found, where classification becomes easy. The lower-dimensional subspace is found with Principal Component Analysis, which identifies the axes with maximum variance. While this kind of transformation is optimal from a reconstruction standpoint, it doesn’t take any class labels into account. Imagine a situation where the variance is generated from external sources, let it be light. The axes with maximum variance do not necessarily contain any discriminative information at all, hence a classification becomes impossible. So a class-specific projection with a Linear Discriminant Analysis was applied to face recognition in [BHK97]. The basic idea is to minimize the variance within a class, while maximizing the variance between the classes at the same time.





I : Créer une bdd de visages (certaines fournies telles que AT&T, Yale...)
II : Les lire depuis un programme (soit csv, soit depuis une arborescence...). Le fichier csv peut etre généré depuis un programme python par exemple.


Algo : 1) Trouver les composantes principales (une image 100*100, c'est 10000 données à traitées, l'idée est de les regrouper par petite région pour trouver les variances principales).
Moyenne, covariance, vecteurs propres, puis trier ces vecteurs propres pour trouver les plus importants
cf eigenface algo


We’ve already seen, that we can reconstruct a face from its lower dimensional approximation. So let’s see how many Eigenfaces are needed for a good reconstruction. I’ll do a subplot with 10,30,\ldots,310 Eigenfaces:
10 Eigenvectors are obviously not sufficient for a good image reconstruction, 50 Eigenvectors may already be sufficient to encode important facial features. You’ll get a good reconstruction with approximately 300 Eigenvectors for the AT&T Facedatabase. There are rule of thumbs how many Eigenfaces you should choose for a successful face recognition, but it heavily depends on the input data. [Zhao03] is the perfect point to start researching for this:

trouve une combinaison linéaire de fonctions qui maximise la variance totale des données . Bien que ce soit clairement un moyen puissant pour représenter des données , il ne considère pas toutes les classes et donc un grand nombre d'informations discriminatoire peut être perdue

FisherFaces : plus sur la dispersion des vecteurs
The Fisherfaces method learns a class-specific transformation matrix, so the they do not capture illumination as obviously as the Eigenfaces method


Probleme des 2 méthodes ? Traite des grosses dimensions d'images, il faut des plus petites régions. LBPH
he Eigenfaces approach maximizes the total scatter, which can lead to problems if the variance is generated by an external source, because components with a maximum variance over all classes aren’t necessarily useful for classification (see http://www.bytefish.de/wiki/pca_lda_with_gnu_octave). So to preserve some discriminative information we applied a Linear Discriminant Analysis and optimized as described in the Fisherfaces method. The Fisherfaces method worked great... at least for the constrained scenario we’ve assumed in our model.



Ce qu'on peut faire aussi c'est voir l'évolution de la reconnaissance de la personne en fonction du nobre d'images dans la bdd

CF LBPH ALGO !!!!

Pour résumer y a 3 méthodes. EigenFaces qui traite de grosse matrice. Qui regarde les vecteurs plus importants. ça marche, mais sensible à la luminosité (donc pas la meilleure en voiture). Fisherface, qui traite aussi de grosses matrices, mais qui se base sur une autre méthode de comparaison de vecteurs ce qui la rend moins sensible à la lumiere et LBPH qui simplifie la matrice en groupant par blocs (et qui elle est super insensible à la luminosité par exemple). J'hsite entre les deux dernières, des tests de rapidité/reconnaissance pourront être pratique dans ce cas


ST3-6 	Photo-Realistic Expressive Text to Talking Head Synthesis, Vincent Wan, Robert Anderson, Art Blokland, Norbert Braunschweiler, Langzhou Chen, BalaKrishna Kolluru, Javier Latorre, Ranniery Maia, Björn Stenger, Kayoko Yanagisawa, Yannis Stylianou, Masami Akamine, Mark J. F. Gales, Roberto Cipolla




Face Recognition with Local Binary Patterns, Spatial Pyramid Histograms and
Naive Bayes Nearest Neighbor classification

Abstract—Face recognition algorithms commonly assume
that face images are well aligned and have a similar pose
– yet in many practical applications it is impossible to meet
these conditions. Therefore extending face recognition to un-
constrained face images has become an active area of research.
To this end, histograms of Local Binary Patterns (LBP)
have proven to be highly discriminative descriptors for face
recognition. Nonetheless, most LBP-based algorithms use a
rigid descriptor matching strategy that is not robust against
pose variation and misalignment.
We propose two algorithms for face recognition that are de-
signed to deal with pose variations and misalignment. We also
incorporate an illumination normalization step that increases
robustness against lighting variations. The proposed algorithms
use descriptors based on histograms of LBP and perform
descriptor matching with spatial pyramid matching (SPM) and
Naive Bayes Nearest Neighbor (NBNN), respectively. Our con-
tribution is the inclusion of flexible spatial matching schemes
that use an image-to-class relation to provide an improved
robustness with respect to intra-class variations.
We compare the accuracy of the proposed algorithms against
Ahonen’s original LBP-based face recognition system and two
baseline holistic classifiers on four standard datasets. Our
results indicate that the algorithm based on NBNN outperforms
the other solutions, and does so more markedly in presence of
pose variations. In this work we will focus on descriptors based on Local
Binary Patterns (LBP), as they are simple, computationally
efficient and have proved to be highly effective features
for face recognition [3], [4], [12], [13]. Nonetheless, the
methods described in this paper can be readily adapted to
operate with alternative local descriptors.
Keywords-face recognition; local binary patterns; naive
Bayes; nearest neighbor; spatial pyramid. In this paper, we propose and compare two algorithms
for face recognition that are specially designed to deal
with moderate pose variations and misaligned faces. These
algorithms are based on previous techniques from the object
recognition literature: spatial pyramid matching [14], [15]
and Naive Bayes Nearest Neighbors (NBNN) [16]. Our
main contribution is the inclusion of flexible spatial match-
ing schemes based on an “image-to-class” relation which
provides an improved robustness with respect to intra-class
variations. These matching schemes use spatially dependent
variations of the “bag of words” models with LBP histogram
descriptors. As a further refinement, we also incorporate
a state of the art illumination compensation algorithm to
improve robustness against illumination changes [17].
I. I NTRODUCTION
Most face recognition algorithms are designed to work
best with well aligned, well illuminated, and frontal pose
face images. In many possible applications, however, it is
not possible to meet these conditions. Some examples are
surveillance, automatic tagging, and human robot interac-
tion. Therefore, there have been many recent efforts to
develop algorithms that perform well with unconstrained
face images [1]–[4].
In this context, the of use local appearance descriptors
such as Gabor jets [5], [6], SURF [7], SIFT [8], [9], HOG
[10] and histograms of Local Binary Patterns [11] have
become increasingly common. Algorithms that use local
appearance descriptors are more robust against occlusion,
expression variation, pose variation and small sample sizes
than traditional holistic algorithms [4], [5].



Within LBP-based algorithms, most of the face recogni-
tion algorithms using LBP follow the approach proposed
by Ahonen et al in [12]. In this approach the face image
is divided into a grid of small of non overlapping regions,
where a histogram of the LBP for each region is constructed.
The similarity of two images is then computed by summing
the similarity of histograms from corresponding regions.

One drawback of the previous method is that it assumes
that a given image region corresponds to the same part of the
face in all the faces in the dataset. This is only possible if the
face images are fully frontal, scaled, and aligned properly. In
addition, while LBP are invariant against monotonic gray-
scale transformations, they are still affected by illumination
changes that induce non monotonic gray-scale changes such
as self shadowing [17].


four main parts:
1) Preprocessing: We begin by applying the Tan and
Triggs’ illumination normalization algorithm [17] to
compensate for illumination variation in the face im-
age. No further preprocessing, such as face alignment,
is performed.
2) LBP operator application: In the second stage LBP are
computed for each pixel, creating a fine scale textural
description of the image.
3) Local feature extraction: Local features are created
by computing histograms of LBP over local image
regions.
4) Classification: Each face image in test set is classified
by comparing it against the face images in the training
set. The comparison is performed using










Facial expression recognition from
video sequences: temporal and static modeling

Since the early 1970s, Paul Ekman and his colleagues [10] have performed exten-
sive studies of human facial expressions. They found evidence to support universality
in facial expressions. These ‘‘universal facial expressions’’ are those representing hap-
piness, sadness, anger, fear, surprise, and disgust.

As mentioned in the previous section, the classifiers used can either be ÔstaticÕ clas-
sifiers or dynamic ones. ÔStaticÕ classifiers use feature vectors related to a single frame
to perform classification (e.g., Neural networks, Bayesian networks, and linear dis-
criminant analysis). Temporal classifiers try to capture the temporal pattern in the
sequence of feature vectors related to each frame such as the HMM based methods
of [23,28,30].

. Cohen et al. / Computer Vision and Image Understanding 91 (2003) 160–187
Fig. 1. (a) The wireframe model and (b) the facial motion measurements.






Image and Vision Computing
We observe in our experiments that LBP features perform stably and robustly over a useful
range of low resolutions of face images, and yield promising performance in compressed low-resolution
video sequences captured in real-world environments.

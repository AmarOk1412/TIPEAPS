Partie 3 : Reconnaissance des émotions 

Dans cette dernière partie , nous allons aborder la reconnaissance des émotions. Comme pour la reconnaissance faciale, nous étudierons d'abord l'aspect théorique, puis nous passerons aux expérimentations.

Partie 1 : Théorie

Avant de passer à l'expérimentation, nous avons besoin de connaitre la méthode la plus adaptée à notre application. C'est pourquoi, dans un premier temps nous allons voir les différentes solutions possibles, puis nous allons choisir celle qui correspond le mieux à nos besoins.

Sous-partie 1 : Différentes solutions

Avant tout chose, nous étudions les expressions faciales, et non directement les émotions. Les expressions faciales sont brèves : entre 250ms et 5s. [cf automaticFacialExpressionAnalysis]. Il existe différentes façons de récupérer des expressions faciales : l'approche holistique (c'est-à-dire que la tête est analysée de façon globale) ou l'approche locale qui consiste à prendre des parties de la tête, et analyser ces parties indépendamment les unes des autres. 

Ensuite, pour "trouver" les émotions, il existe également différentes manières de procéder. Nous avons l'approche basée sur un modèle : à partir d'une image, //-- à compléter --//. Nous avons aussi l'approche basée sur l'image : à partir d'une image, nous essayons de trouver des expressions faciales qui s'apparentent aux émotions primaires définies par Ekman & Friesen. Touts les humains ont la même façon d'exprimer ces émotions primaires, peu importe son origine ou sa culture [cf 2002ElfenbeinMeta - On the Universality and cultural Specificity of Emotion Recognition: A Meta-Analysis]. Cette méthode présente l'avantage d'être plus simple et plus rapide à mettre en place. Cependant, elle est moins robuste, notamment aux changements de position de la tête. 

Enfin, pour visualiser les changements //--à préciser--//, nous pouvons regarder les déformations. Pour commencer, cela consiste à classer les émotions primaires en termes de mouvements, translations, rotations, etc. Puis, nous prenons des images à différents instants, et nous analysons les mouvements, translations, rotations, etc afin de les comparer à ceux des émotions primaires. Une autre solution est le suivi des points du visage : sur chaque image, nous relevons la position de certains points, et nous essayons de les aligner sur le modèle de chaque expression primaire. Finalement, l'émotion retenue est celle dont le modèle de points est le plus proche de celui de l'image.

Sous-partie 2 : Solution choisie

Avant de présenter la solution choisie, nous avons testé plusieurs autres méthodes que nous avons abandonné. Par exemple, pour détecter la forme de la bouche, nous avons utilisé une fonction présente dans OpenCV : "findContours" //-- (insérer l'image dans image + code en annexe dans data)--// qui permet d'obtenir des résultats plutôt bons //-- image fin--//. Cependant, dans certains cas, le résultat n'était pas du tout exploitable //--image fin3--//. Par ailleurs, la détection des couleurs //--exemple de code dans data--// a donné de bien meilleurs résultats //--détection de la couleur de la bouche : image fin2.jpg + code dans data--//. Utiliser "findContours" aurait été plus précis, mais son manque de fiabilité et la nécessité de traiter les images rend son utilisation contraignante. Une autre solution consistait à détecter les expressions à l'aide des couleurs et de l'utilisation de seuils spécifiques. C'était la solution la plus rapide et la plus simple à mettre en place ##need précisions : en quoi ça consiste; pourquoi nous l'avons abandonnée si elle rapide et simple, etc.##

La solution choisie est une méthode légèrement différente de celle que nous avons présentée ##laquelle ?##. Le suivi des points ou les réseaux neuronnaux ont pour désavantage de demander énormément de calculs. Sachant que notre application a pour support l'ordinateur de bord d'une voiture (vraisemblablement moins puissant qu'un ordinateur portable), il est nécessaire de trouver une solution portable. Pour simplifier les calculs, voici la méthode utilisée pour la reconnaissance des émotions : la première étape est l'obtention d'un visage. L'identité de la personne nous importe peu (elle est uniquement utilisée pour le démarrage du véhicule). Ensuite, à l'aide d'un second classifier, nous obtenons toutes les positions suceptibles de correspondre à des yeux. Les yeux situés dans la partie basse du visage sont écartés. La hauteur du classifier nous indique si les yeux sont normaux, froncés ou écarquillés. Pour détecter les yeux fermés, il suffit de regarder le nombre de pixels ayant une couleur comprise entre (80,0,0) et (160,100,100). Concernant la bouche, la méthode est presque identique. Avec un classifier spécifique, toutes les bouches possibles sont récupérées, puis les erreurs sont écartées (bouches situées dans la partie haute du visage ou trop près du bord). Ensuite, la bouche est convertie en niveaux de gris //-- c-à-d ?--//  et un seuil est appliquée pour n'obtenir que l'intérieur de la bouche. //--Si y a de l'intérieur au-dessus du seuil de la bouche fermée c'est quelle est ouverte. --> explique différemment stp :) --//

Après cet aperçu des différentes solutions possibles pour la reconnaissance faciale et du choix de notre méthode, nous allons à présent l'expérimenter.

Partie 2 : Expérimentations

Dans cette partie, nous allons présenter les expérimentations, nous allors d'abord exposer le protocole expérimental, puis la réalisation en elle-même, et enfin les conclusions que nous en avons tiré.

Sous-partie 1 : Protocole

Dans le but de régler les seuils de façon précise, il a fallu définir une distance entre le sujet et l'objectif de la caméra (cette distance a été fixée à 50 cm), et effectuer une initialisation avec une émotion neutre. Puis, pour vérifier la robustesse de la détection, le sujet devait froncer les sourcils, écarquiller et fermer les yeux, et ouvrir la bouche. Ces déformations particulières du visage correspondent aux expressions faciales (la colère, la surprise et l'endormissement) que nous souhaitons récupérer pour notre application. Le processus est répétée jusqu'à la détection de l'expression faciale.

Sous-partie 2 : Réalisation

//--A COMPLETER--//

Sous-partie 3 : Résultats et analyse

//--à l'arrêt, avec une luminosité ne changeant pas énormément, la détection marchait bien. Juste des problemes dès qu'on avançait/reculait --> pourquoi à l'arrêt ? l'expérience dans la voiture, ce n'est pas plutôt pour la prod finale ?--//



Partie 3 : Reconnaissance des émotions 

Dans cette dernière partie, nous allons aborder la reconnaissance des émotions. Comme pour la reconnaissance faciale, nous étudierons d'abord l'aspect théorique, puis nous passerons aux expérimentations.

Partie 1 : Théorie

Avant de passer à l'expérimentation, nous avons besoin de connaitre la méthode la plus adaptée à notre application. C'est pourquoi, dans un premier temps nous allons voir les différentes solutions possibles, puis nous allons choisir celle qui correspond le mieux à nos besoins.

Sous-partie 1 : Différentes solutions

Avant toute chose, nous étudions les expressions faciales et non directement les émotions. Les expressions faciales sont brèves : entre 250ms et 5s. [cf automaticFacialExpressionAnalysis]. Il existe différentes façons de récupérer des expressions faciales : l'approche holistique (c'est-à-dire que la tête est analysée de façon globale) ou l'approche locale qui consiste à prendre des parties de la tête et analyser ces parties indépendamment les unes des autres. 

Ensuite, pour "trouver" les émotions, il existe également différentes manières de procéder. Nous avons l'approche basée sur un modèle : à partir d'une image, //-- à compléter --////--SEB : En gros, c'est pas comme ce qu'on a fait pour reconnaitre un visage, mais à la place de reconnaitre une personne, c'était une émotion. Genre dans la BDD on aurait Seb content, seb pas content, seb dodo, ...--//. Nous avons aussi l'approche basée sur l'image : à partir d'une image, nous essayons de trouver des expressions faciales qui s'apparentent aux émotions primaires définies par Ekman & Friesen. Tous les humains ont la même façon d'exprimer ces émotions primaires, peu importe son origine ou sa culture [cf 2002ElfenbeinMeta - On the Universality and cultural Specificity of Emotion Recognition: A Meta-Analysis]. Cette méthode présente l'avantage d'être plus simple et plus rapide à mettre en place. Cependant, elle est moins robuste, notamment aux changements de position de la tête. 
//--METTRE IMAGE DES 6 émotions de base--//

Enfin, pour visualiser les changements //--à préciser--//, nous pouvons regarder les déformations. Pour commencer, cela consiste à classer les émotions primaires en termes de mouvements, translations, rotations, etc. Puis, nous prenons des images à différents instants et nous analysons les mouvements, translations, rotations, etc... afin de les comparer à ceux des émotions primaires. Une autre solution est le suivi des points du visage : sur chaque image, nous relevons la position de certains points et nous essayons de les aligner sur le modèle de chaque expression primaire. Finalement, l'émotion retenue est celle dont le modèle de points est le plus proche de celui de l'image. C'est la méthode la plus robuste, mais aussi la plus couteuse en calcul et plus difficile à mettre en place que la précédente méthode.

Sous-partie 2 : Solution choisie

Avant de présenter la solution choisie, nous avons testé plusieurs autres méthodes que nous avons abandonné. Par exemple, pour détecter la forme de la bouche, nous avons utilisé une fonction présente dans OpenCV : "findContours" //-- (insérer l'image dans image + code en annexe dans data)--// qui permet d'obtenir des résultats plutôt bons //-- image fin--//. Cependant, dans certains cas, le résultat n'était pas du tout exploitable //--image fin3--//. Par ailleurs, la détection des couleurs //--exemple de code dans data--// a donné de bien meilleurs résultats //--détection de la couleur de la bouche : image fin2.jpg + code dans data--//. Utiliser "findContours" aurait été plus précis, mais son manque de fiabilité et la nécessité de traiter les images rend son utilisation contraignante. Une autre solution consistait à détecter les expressions à l'aide des couleurs et de l'utilisation de seuils spécifiques. C'était la solution la plus rapide et la plus simple à mettre en place, mais elle est imprécise et difficile à utiliser pour savoir si les yeux sont froncés ou écarquillés par exemple.

La solution choisie est une méthode légèrement différente de la méthode de détection de couleur. Le suivi des points ou les réseaux neuronaux ont pour désavantage de demander énormément de calculs. Sachant que notre application a pour support l'ordinateur de bord d'une voiture //-à reformuler--// (vraisemblablement moins puissant qu'un ordinateur portable), il est nécessaire de trouver une solution portable. Pour simplifier les calculs, nous avons défini la méthode suivante pour effectuer la reconnaissance des émotions : la première étape est l'obtention d'un visage. L'identité de la personne nous importe peu (elle est uniquement utilisée pour le démarrage du véhicule). Ensuite, à l'aide d'un second classifieur, nous obtenons toutes les positions susceptibles de correspondre à des yeux. Les yeux situés dans la partie basse du visage sont éliminés. La hauteur du classifieur nous indique si les yeux sont en position normale, froncés ou écarquillés. Pour détecter les yeux fermés, il suffit de regarder le nombre de pixels ayant une couleur comprise entre (80,0,0) et (160,100,100) (on effectue donc une détection par la couleur pour ce point). Concernant la bouche, la méthode est presque identique à celle des yeux. Avec un classifier spécifique, toutes les bouches possibles sont récupérées, puis les erreurs sont écartées (bouches situées dans la partie haute du visage ou trop près du bord). Ensuite, la bouche est convertie en niveaux de gris  et un seuil est appliquée pour n'obtenir que l'intérieur de la bouche. En effet, on peut remarquer que la couleur à l'intérieur d'une bouche ouverte tend vers le noir, alors que les lèvres sont plus d'une couleur rouge. Ainsi, on a plus qu'à comparer le taux de noir de l'image au taux obtenu à l'initialisation du programme. Si le taux de noir de l'image est supérieur à celui obtenu à l'initialisation plus un seuil, la bouche est ouverte. 

Après cet aperçu des différentes solutions possibles pour la reconnaissance faciale et du choix de notre méthode, nous allons à présent l'expérimenter.

Partie 2 : Expérimentations

Dans cette partie, nous allons présenter les expérimentations, nous allors d'abord exposer le protocole expérimental, puis la réalisation en elle-même, et enfin les conclusions que nous en avons tiré.

Sous-partie 1 : Protocole

Dans le but de régler les seuils de façon précise, il a fallu définir une distance entre le sujet et l'objectif de la caméra (cette distance a été fixée à 50 cm), et effectuer une initialisation avec une émotion neutre. Puis, pour vérifier la robustesse de la détection, le sujet devait froncer les sourcils, écarquiller et fermer les yeux et ouvrir la bouche. Ces déformations particulières du visage correspondent aux expressions faciales (la colère, la surprise et si le conducteur était endormi) que nous souhaitons récupérer pour notre application. Le processus est répété jusqu'à la détection de l'expression faciale.

Sous-partie 2 : Réalisation

//--A COMPLETER--//
//--SEB: Tu veux mettre quoi à part qu'on a réaliser le protocole--//

Sous-partie 3 : Résultats et analyse
//--REFORMULATION : Dans un lieu où l'utilisateur de l'application ne bouge pas et où la luminosité ne change pas, on arrive à obtenir une détection correcte des émotions. On pouvait tout de même observer des problèmes lorsque l'utilisateur s'éloignait ou reculait de la caméra. En effet les seuils et les tailles devenaient incorrects. Il fallait donc réinitialiser les seuils.--//
//--à l'arrêt, avec une luminosité ne changeant pas énormément, la détection marchait bien. Juste des problemes dès qu'on avançait/reculait --> pourquoi à l'arrêt ? l'expérience dans la voiture, ce n'est pas plutôt pour la prod finale ?--//



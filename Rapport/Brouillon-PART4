Partie 4 : Production 

Dans cette dernière partie, nous présenterons la réalisation de notre application. Nous parlerons d'abord du prototype final, puis des tests réalisés en situation réelle et enfin, des résultats de ces tests.

Part 1 : Prototype final 

Afin de vérifier l'efficacité de notre application (voir code source en annexe), nous avons décidé de la tester en situation réelle, c'est-à-dire dans un véhicule en mouvement. Cependant, nous n'avions pas les connaissances et le matériel pour relier directement notre programme à un véhicule et modifier en temps réel son comportement (le freinage par exemple) //--(ni l'autorisation)--//. C'est pourquoi, à l'aide d'un jeu de DELs et d'un montage électronique réalisé à l'aide d'une carte Arduino Uno (voir le schéma), nous avons simulé les différentes réactions du véhicule en fonction du comportement du conducteur. Ainsi, nous avons choisi d'utiliser quatres DELs :
_ Une DEL rouge pour simuler le frein
_ Une DEL orange clignotante pour simuler les warnings
_ Une DEL jaune pour simuler une limitation de l'accélération
_ Une DEL verte pour simuler le droit de démarrer
De plus, une enceinte était relié à l'Arduino pour émettre un signal sonore en cas d'inattention de la part du conducteur ou de son endormissement. Le prototype est fonctionnel sous Linux, Windows, et Mac OS (il est à noter qu'il y a quelques problèmes de dépendances liés à BSD//--PAS plutôt OpenCV ??????--//). Par ailleurs, nous n'avons pas réussi à porter l'application sur les cartes électroniques Rasberry Pi et Beaglebone à cause d'une fonction non-supportée pour le moment (createLBPHFaceRecognizer qui permet l'utilisation de la méthode de reconnaissance faciale LBPH) //--que nous n'avons pas regardé en détail par manque de temps--//. 

Part 2 : Protocole du test final 

Tout d'abord, la première partie du test était réalisée sur le passager avant. Nous avons effectué l'initialisation à l'arrêt, ainsi qu'une détection des émotions pour vérifier le fonctionnement correct de l'application. Puis, un trajet prédéfini //--en fait pas du tout :D, improvisé surtout--// a été réalisé pour obtenir trois endroits avec des luminosités différentes (faible, naturelle et forte) afin de vérifier la robustesse du programme. Lors de la seconde partie du trajet, une caméra a été placée en face du conducteur (au niveau du volant pour ne pas gêner sa visibilité). Nous avons roulé de façon continue pour simuler une situation réelle et confirmer le bon fonctionnement de l'application et visualiser d'éventuels problèmes.

Part 3 : Discussion et analyses des résultats

Premièrement, l'application fonctionne bien dans le cas où le conducteur est inattentif. //--Cependant, le temps de réponse est assez long (entre 5 et 10 secondes) SEB:perso je trouve pas, sinon ça ferait chier pendant les angles mort et tout--//. Quand nous avons simulé un conducteur endormi, la détection s'effectuait dans un temps compris entre 5 et 15 secondes //--SEB: la ok--//. Le programme marche également pour les expressions faciales "énervé" et "surpris" mais comme précédemment, la réaction du programme n'est pas instantanée et les émotions doivent être sur-jouée la plupart du temps. Nous en avons conclu que la qualité de la caméra était à revoir, ainsi que son placement dans l'habitacle. En effet, le volant empêchait la caméra de bien visualiser le visage du conducteur. Le point le plus problématique concerne le changement de luminosité. Lorsque la luminosité est trop importante, le visage du conducteur devient blanc et se confond avec l'arrière-plan. Il est alors impossible d'identifier une quelconque expression faciale, ni même de reconnaître le visage.

Nous venons donc de voir qu'une intégration de notre prototype est possible dans une voiture de tourisme. L'application détecte bien les expressions recherchées malgré un temps de réponse important. Cependant, plusieurs paramètres doivent encore être améliorés comme une meilleure adaptation au changement de luminosité ou le choix d'une caméra avec une résolution plus élevée.



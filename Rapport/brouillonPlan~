Page de garde

* Remerciements
* Abstract
* Sommaire
* Introduction
  ** Pourquoi ce projet
     *** Petite partie sur les projets identiquesPROJETS SIMILAIRES
         .http://www.ictjournal.ch/fr-CH/News/2013/11/08/KeyLemon-entre-dans-votre-voiture.aspx
La caméra regarde où regarde le chauffeur. Et un radar est utilisé pour détecter les piétons. Identifie le conducteur et règle le siège, rétro, ...
         .http://www.gentside.com/denso/un-systeme-de-reconnaissance-faciale-detecte-la-somnolence-au-volant_art31705.html
Denso détecte si le conducteur s'endort et possède 6 phases de notifications. La détection se fait à l'aide de 17 points sur le visage
         .http://www.lemondeinformatique.fr/actualites/lire-ceatec-la-voiture-de-demain-diagnostiquera-le-conducteur-55202.html
Alps Electric comme le premier projet. Identifie le conducteur et règle + bilan de santé + reconnaissance de la main
         .http://www.generation-nt.com/toyota-fv2-concept-voiture-capable-lire-emotions-conducteur-actualite-1810542.html
change la couleur du véhicule selon l'humeur du conducteur + pare brise avec réalité augmentée
  ** Histoire de la reconnaissance Faciale
     *** Ekman
* Reconnaissance Faciale
  ** Théorie
     *** Général
         . Enormément de méthodes, on peut aussi combiner avec des méthodes telles que la mise en place d'un réseau neuronnal (mais en 3 mois, pas le temps).
         . On traite des images en n&b car la colorimétrie n'est pas importante.
     *** Eigenface
         . Définition :
Eigenface -> se base sur les eigen vectors : vecteurs propres
         . Algo
Chaque image est considéré comme un vecteur de dimension son nombre de pixels.
Puis un/des algos recherche(nt) les principales composantes.
A l'aide de plusieurs images du même visage on peut alors composer une image de visage moyen qui contiendra elle aussi les principales composantes (l'eigenface).
La méthode de reconnaissance des axes principaux peut être trouvée ici :
Matthew Turk and Alex Pentland. Eigenfaces for recognition.
J. Cognitive Neuroscience
,
3(1) :71{86, 1991}
Il suffit ensuite de comparer l'eigenface et les composantes principales de la capture.

TODO ALGO MATH ?
         . Avantages/inconvénients
. Comme chaque pixel est une dimension, une image 100*100 donne autant de vecteurs à traiter. Ce qui est énorme.
. sensible à la lumière, à la déformation des visages
. Avantage : connue depuis longtemps.
     *** FisherFace
         . Algo
L'algo fisherface se base sur les travaux  de  Sir R. A. Fisher.
L'idée est simple. Tout ce qui se ressemble se retrouvent proche.
Si on imagine un espace d'image. Et une échelle représentant les visages.
Si on projette les images sur l'échelle, On peut donc dire que ce visage ressemble le plus à celui qui est à côté dans l'échelle.
Figure présente dans le dossier image
Plus mathématiquement :http://docs.opencv.org/modules/contrib/doc/facerec/facerec_tutorial.html#fisherfaces
         . Avantages/inconvénients
. Comme chaque pixel est une dimension, une image 100*100 donne autant de vecteurs à traiter. Ce qui est énorme.
. moins sensible à la lumière, à la déformation des visages que eigenface car ne se base pas sur les composantes discriminatoires
. Avantage : connue depuis longtemps.
     *** LBPH
         . LBPH : Local Binary Patterns Histogram
         . Description :
Consiste à regarder le niveau d'un pixel par rapport à ses voisins. 
La première étape consiste à diviser l'image en groupes de pixels. 
Le pixel choisit sert de base pour effectuer un seuil avec les voisins (grâce à la fonction d'Heaviside. Qui dit 0 si x<0, 1 sinon. Ici on a 0 si x<pixel de base 1 sinon). 
Puis une pondération de chaque pixel donne une valeur.
Chaque groupe est passé à cette pondération. 
Toutes les valeurs sont mises les unes à la suite des autres pour former l'histogramme de l'image.
Il ne reste plus qu'à faire la différence entre deux histogrammes.
         . Un algo très utilisé
Cette algo est très utilisé comme dans :
TITRE: 
Facial expression recognition based on Local Binary Patterns:A comprehensive study
AUTEURS:
Caifeng Shan a, * , Shaogang Gong b , Peter W. McOwan b
Ou il est combiné avec Adaboost pour obtenir de meilleurs résultats + un réseau neuronnal
Ou TITRE:
Face Recognition with Local Binary Patterns, Spatial Pyramid Histograms and
Naive Bayes Nearest Neighbor classification
AUTEURS:
Daniel Maturana, Domingo Mery and Alvaro
Soto
où les auteurs essayent d'améliorer l'algo pour obtenir le leur : NNBN que nous n'utiliserons pas (temps, et LBPH est déjà intégré dans OpenCV)
  ** Expérimentations
     *** Protocole
         . Le but de cette expérimentation était de comparer les 3 méthodes implémentées par la librairie OpenCV pour voir laquelle on pouvait choisir pour l'application finale puis de voir la robustesse de cette méthode.
         . On a donc réaliser 2 expériences. La première pour regarder la vitesse selon le nombre de photos de bases. Puis de comparer le %age de réussite.
         . La seconde pour regarder la robustesse (grimaces, visage caché, rotation, ...).
     *** Réalisation Expérience 1
         . On a regardé deux vitesses. Le temps de la construction du modèle puis le temps pour réaliser la reconnaissance.
         . On a donc capturé une photo pour servir de base. La base de données était composée de 3 individus.
         . Puis pour chaque algo, et pour éviter au max les impressisions due au processeur qui ne travaille pas toujours pareil on a passé l'image pour chaque algo 100 fois (puis on a fais la moyenne pour avoir le temps moyen) sur le même ordinateur.
         . On a donc testé 3 algos (Eigenface, fisherface, LBPH) avec une image de base, avec une base de données de 3 individus avec le nombre de photo suivant : 1,2,4,6,8,10,12.
     *** Résultats & Analyse Expérience 1
         . Refaire les graphes avec latex
         . Pour le temps de création du modele. FisherFace est le plus rapide. Puis EigenFace (qui deviens de plus en plus lent jusqu'à rattraper LBPH au bout de 36 photos). LBPH reste le plus lent.
         . Pour la reconnaissance des visages, la durée reste la même avec environ 680<->690 ms avec un processeur i5 2 eme génération pour les 3 algos.
         . Ainsi on remarque que le temps n'est pas le principal facteur. Si on prend LBPH, l'initialisation du système sera un peu plus longue, mais le temps de reconnaissance restera identique le reste de l'application.
     *** Réalisation Expérience 2
         . Pour déterminer le meilleur algorithme, on a donc :
         . Pris une vidéo de 100 frames ou l'individu bouger la tête.
         . Enregistré l'individu avec 12 photos dans la base de données.
         . Passé la vidéo aux 3 algos pour voir le %age de reconnaissance.
     *** Résultats & Analyse Expérience 2
         . Refaire les graphes avec latex
         . Amaury : Eigenface = 71.929824561404% / fisherface = 59.649122807018% / LBPH = 82.456140350877%
Phenri : Eigenface = 7.4074074074074% / fisherface = 80.246913580247% / LBPH = 60.493827160494%
Seb : Eigenface = 24.657534246575% / fisherface = 73.972602739726% / LBPH = 90.41095890411%
         . On remarque que LBPH est globalement le meilleur des 3. Puis vient FisherFace puis EigenFace loin derrière.
     *** Réalisation Expérience Robustesse
         . On a donc choisit LBPH
         . Pour la robustesse on a demandé à plusieurs personnes de s'enregistrer dans la base de données.
         . Puis de prendre une vidéo de 100 frames.
         . On a donc pris différente personne sous :
           .. différentes luminosité (élevée, naturelle, faible)
           .. différentes orientation de la tête
           .. différentes inclinaison
           .. différentes déformation du visage
           .. différentes parties cachées du visage   
           .. différentes distances
     *** Résultats & Analyse Robustesse
!!!!!!BESOIN DE PHOTOS, JE M'EN OCCUPE (SEB) + PH SI TU AS ES PARTIES DU VISAGE CACHEES!!!!!!!!       
         . On en a donc déduit
           .. différentes luminosité (élevée, naturelle, faible) : La luminosité peut être un probleme lorsque les yeux sont trop cachés par la lumièce ou lorsque la limite face/décor est difficile à voir).
           .. différentes orientation de la tête (limite d'environ 20°, il faut au moins garder les deux yeux bien visible)
           .. différentes inclinaison (idem)
           .. différentes déformation du visage (pas de problème). Des accessoires telles que des lunettes ne changent pas beaucoup le résultat, mais le mieux reste de prendre des photos dans la base de données avec plusieurs accessoires.
           .. différentes parties cachées du visage (réussite amoindrie lorsque l'on cache la bouche mais réalisable). Si on cache les yeux, la reconnaissance est impossible (embettant pour les cheveux longs)
           .. différentes distances (On perd énormément de réussite lorsque la taille du visage reconnue est plus petite que celle dans la base de données).
           .. La réalisation d'autres classifiers pour améliorer la reconnaissance pourrait être intéressant.
* Reconnaissance des émotions
  ** Théorie
<<<<<<< HEAD
     *** Solutions abandonnées
         . Avant de choisir la solution présente plus loin, on a testé plusieurs autres méthodes pour détecter la forme de la bouche.
           .. Find Contours (insérer l'image dans image + code en annexe dans data). Qui permet d'avoir des résultats assez bon (image fin), mais qui pouvait aussi donner n'importe quoi (image fin3). Par contre la détection de couleur (exemple de code dans data) a donné de meilleur résultat (détection de la couleur de la bouche : image fin2.jpg + code dans data).
FindContours aurait été plus précis, mais peu fiable et nécessite pas mal de traitement derrière. La couleur avec des seuils bien spécifiques était la solution la plus rapide + la plus simple.
=======
         . /!\ : on étudie les expressions faciales, et non directement les émotions.
         . les expressions faciales sont breves (entre 250ms et 5s) [cf automaticFacialExpressionAnalysis]
     *** Différentes solutions
TODO     . contours bouche, couleur, ...
         Pour récupérer les emotions :
         . Approche holistique (= prendre comme un tout)
            -> prendre toute la tete et regarder ce qui bouge
         . Approche locale (ce qu’on fait)
            -> prendre juste des parties de la tete, et analyser chaque partie indépendamment des autres
        Pour « trouver » les emotions :
         . Approche basée sur un modèle :
            -> prendre l’image, la calquer sur un modèle et comparer ce qui se passe à comment le modèle réagit en fonction de l’émotion [ex: authenticFacialExpressionAnalysis]
         . Approche basée sur l’image (nous) :
            -> On regarde l’image, et on essaye de trouver ce qui s’apparente aux émotions primaires de Ekman & Friesen (qui sont constantes sur tous les continents, peu importe les différences culturelles [cf 2002ElfenbeinMeta - On the Universality and cultural Specificity of Emotion Recognition: A Meta-Analysis])
            -> plus rapide et simple à mettre en place, mais moins robuste (notamment aux changements de positions de la tête)
        Pour voir ce qui change :
         . regarder les déformations
            -> on classe les émotions primaires en termes de mouvements, translations, rotations, etc.
            -> on regarde les images à différents moments, et on analyse les mouvements, translations, rotations, etc.
         . Suivre des points du visage (nous)
            -> on regarde comment sont les points sur chaque image, et on essaye de les calquer aux points sur les données de chaque expression primaire.
            -> on prend ‘emotion la plus proche
>>>>>>> 93e0528ba7f9aa79e2f5272ae3c0cd36380cc176
     *** Solution choisie
         . La solution qu'on a choisie est une méthode un peu différente de celle présentée. Le tracking de points ou les réseaux neuronaux demandent énormément de calcul. On doit se baser dans le cas d'une voiture, donc d'un ordinateur de bord, sans doute moins puissant qu'un pc de bureau. Pour simplifier les calculs voilà la méthode qu'on utilise pour la reco des émotions :
         . On obtient le visage. L'identité de la personne ne nous intéresse pas (on vérifie si la personne est autorisée au départ)
         . Avec un second classifier, on obtient la position des yeux. On enlève tous ceux que l'application peut trouver dans la partie basse du visage qui correspond à des erreurs. Pour ces yeux on récupère la hauteur du classifier pour savoir si les yeux sont froncés, équarquillés ou normaux). Pour détecter les yeux fermés, On regarde le nombre de pixels ayant une couleur entre (80, 0,0), (160,100,100).
         . Pour la bouche, la méthode est presque identique. Avec un classifier on récupère les bouches disponibles et on enlève les bouches impossibles (celles dans la partie haute du visage ou trop prêt du bord). Puis on converti la bouche en niveaux de gris et on effectue un seuil pour n'obtenir que l'intérieur de la bouche. Si y a de l'intérieur au-dessus du seuil de la bouche fermée c'est quelle est ouverte.
  ** Expérimentations
     *** Protocole
         . Pour le réglage des seuils, il a fallu placer un sujet à une distance prédéfinie (50cm), initialiser les seuils avec une émotion neutre. Puis pour vérifier la détection :
_ Froncer les sourcils
_ Equarquiller les yeux
_ Fermer les yeux
_ Ouvrir la bouche
Ce sont les choses à vérifier pour détecter la colère, la surprise et l'endormissement qu'on voulait.
Recommencer jusqu'à réussir la détection.
     *** Résultats & Analyse
à l'arrêt, avec une luminosité ne changeant pas énormément, la détection marchait bien. Juste des problemes dès qu'on avançait/reculait
* Production
  ** Prototype Final réalisé
     . Dire que le code est dispo en annexe
     . Schéma de l'arduino
     . Le prototype est fonctionnel sous linux/windows/mac (quelques problèmes de dépendances)/BSD)
     . Le prototype n'est pas fonctionnel sous Rasbpy ni Beaglebone à cause d'une fonction non portée (createLBPHFaceRecognizer), et on avait pas le temps de se pencher sur le probleme.
  ** Tests finaux
     *** Protocole
     L'initialisation c'est faite au départ. Pour la première partie du trajet, l'application fonctionnait sur le passager.
La détection des émotions au départ a été réalisée pour voir le fonctionnement correct de l'application.
Puis, un trajet a été réalisé pour trouver 3 points différents (1 avec forte luminosité, 1 luminosité naturelle, 1 dans un endroit sombre) pour voir si l'application fonctionnait correctement.
Pour la seconde partie du trajet, une caméra a été placée devant le conducteur (niveau volant pour ne pas géner la vue). Puis le retour c'est effectué pour voir si dans une application pratique, l'application marchait correctement et s'il y avait des problèmes à signaler.
  ** Discussion & Analyse des résultats
     . Marche pour inatentif
     . Lent pour endormi (5/15 secondes)
     . Marche pour énervé & surpris, mais lent.
     . Qualité de la caméra à revoir
     . Placement de la caméra aussi, le volant gêne beaucoup
     . Ne supporte pas les changements de luminosité (le conducteur devient tout blanc, dur de trouver le visage), pour les virages.
     . TODO CONTINUER
* Conclusion
  ** Bilan/synthèse
     *** Réponse à la problématique
     *** Compétences acquises
         . travail de groupe
         . techniques de communication
         . git
         . python 2 car python 3 mal supporté pour la reconnaissance faciale d'opencv
         . opencv
         . php
  ** Ouverture
     *** Utilité
         . Plus comme gadget, endormi/inattentif/reconnaissance du conducteur pour éviter un vol, couleur en fonction de l'humeur, même si reconnaissance faible (une photo suffit pour contourner le systeme), les émotions sont peu fiables car peuvent provenir de beaucoup de sources.
         . Compléter plus avec radars/caméras vers l'extérieur.
--- Annexes ---
* Code des applications
* Bibliographie


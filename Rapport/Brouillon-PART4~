Partie 4 : Production 

Dans cette dernière partie, nous présenterons la réalisation de notre application. Nous parlerons d'abord du prototype final, puis des tests réalisés en situation réelle et enfin, des résultats de ces tests.

Part 1 : Prototype final 

Afin de vérifier l'efficacité de  notre application (voir code source en annexe), nous avons décidé de la tester en situation réelle, c'est-à-dire dans un véhicule en mouvement. Cependant, nous n'avions pas les connaissances et le matériel pour relier directemement notre programme à un véhicule et modifier en temps réel son comportement (le freinage par exemple). C'est pourquoi, à l'aide d'un jeu de LEDs et d'un circuit imprimé de type Arduino Uno (voir le schéma), nous avons simulé les différentes réactions du véhicule en fonction du comportement du conducteur. Ainsi, nous avons choisi d'utiliser quatres LEDs : //-- mettre la correspondance entre la couleur et sa signification merci :) --//. De plus, une enceinte était relié à l'Arduino pour émettre un signal sonore en cas d'inattention du conducteur. Le prototype est fonctionnel sous Linux, Windows :), et Mac OS (il est à noter qu'il y a quelques problèmes de dépendances liés à BSD). Par ailleurs, nous n'avons pas réussi à porter l'application sur les cartes électroniques Rasberry Pi et Beaglebone à cause d'une fonction non-supportée (createLBPHFaceRecognizer qui permet l'utilisation de la méthode de reconnaissance faciale LBPH //--c'est ça ?--//). 

Part 2 : Protocole du test final 

Tout d'abord, la première partie du test était réalisée sur la passager avant. Nous avons effectuer l'initialisation à l'arrêt, ainsi qu'une détection des émotions pour vérifier le fonctionnement correct de l'application. Puis, un trajet prédéfini a été réalisé pour obtenir trois endroits avec des luminosités différentes (faible, naturelle et forte) afin de vérifier la robustesse du programme. Lors de la seconde partie du trajet, une caméra a été placée en face du conducteur (au niveau du volant pour ne pas gêner sa viibilité). Nous avons roulé de façon continue pour simuler une situation réelle et confirmer le bon fonctionnement de l'application et visualiser les éventuels problèmes.

Part 3 : Discussion et analyses des résultats

Premièrement, l'application fonctionne plutôt bien dans le cas où le conducteur est inattentif. Cependant, le temps de réponse est assez long (entre 5 et 10 secondes). Quand nous avons simulé un conducteur endormi, il était compris entre 5 et 15 secondes. Le programme marche également pour les expressions faciales "énervé" et "surpris" mais comme précedemment, la réaction du programme n'est pas instantannée. Nous en avons conclu que la qualité de la caméra était à revoir, ainsi que son placement dans l'habitacle. En effet, le volant empêchait la caméra de bien visualiser le visage du conducteur. Le point le plus problématique concerne le changement de luminosité. Lorsque la luminosité est trop importante, le visage du conducteur devient tout blanc et se confond avec l'arrière-plan. Il est alors impossible d'identifier une quelconque expression faciale.

Nous venons donc de voir qu'une intégration de notre prototype est possible dans une voiture de tourisme. L'application détecte bien les expressions recherchées malgré un temps de réponse important. Cependant, plusieurs paramètres doivent encore être améliorés commme une meilleur adaptation au changement de luminosité ou le choix d'une caméra avec une résolution plus élevée.


